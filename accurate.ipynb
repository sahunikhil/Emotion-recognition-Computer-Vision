{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"accurate.ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"11KGY5kCSTQtHbHf4dD0PCaIRQQOw6RAH","authorship_tag":"ABX9TyPXAuzTKcWWUGezESegBBCV"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"QM5iFOGupqjk","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":69},"outputId":"c17b72dd-406a-4d47-e6d8-294fc93898fb"},"source":["# USAGE\n","# python gradient_tape_example.py\n","\n","# import the necessary packages\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import BatchNormalization\n","from tensorflow.keras.layers import Conv2D\n","from tensorflow.keras.layers import MaxPooling2D\n","from tensorflow.keras.layers import Activation\n","from tensorflow.keras.layers import Flatten\n","from tensorflow.keras.layers import Dropout\n","from tensorflow.keras.layers import Dense\n","from tensorflow.keras.optimizers import Adam\n","from tensorflow.keras.losses import categorical_crossentropy\n","from tensorflow.keras.utils import to_categorical\n","from keras.preprocessing.image import ImageDataGenerator \n","#from tensorflow.keras.datasets import mnist\n","import tensorflow as tf\n","import numpy as np\n","import time\n","import sys\n","\n","Classes = ['angry','happy','surprised','disgusting','sad','fear']\n","trainY = Classes\n","testY = Classes\n","\n","img_width, img_height = 224, 224\n","train_data_dir = 'data/train'\n","validation_data_dir = 'data/test'\n","nb_train_samples = 1200\n","nb_validation_samples = 300\n","epochs = 25\n","batch_size = 64\n","\n","def build_model(width, height, depth, classes):\n","\t# initialize the input shape and channels dimension to be\n","\t# \"channels last\" ordering\n","\tinputShape = (height, width, depth)\n","\tchanDim = -1\n","\n","\t# build the model using Keras' Sequential API\n","\tmodel = Sequential([\n","\t\t# CONV => RELU => BN => POOL layer set\n","\t\tConv2D(16, (3, 3), padding=\"same\", input_shape=inputShape),\n","\t\tActivation(\"relu\"),\n","\t\tBatchNormalization(axis=chanDim),\n","\t\tMaxPooling2D(pool_size=(2, 2)),\n","\n","\t\t# (CONV => RELU => BN) * 2 => POOL layer set\n","\t\tConv2D(32, (3, 3), padding=\"same\"),\n","\t\tActivation(\"relu\"),\n","\t\tBatchNormalization(axis=chanDim),\n","\t\tConv2D(32, (3, 3), padding=\"same\"),\n","\t\tActivation(\"relu\"),\n","\t\tBatchNormalization(axis=chanDim),\n","\t\tMaxPooling2D(pool_size=(2, 2)),\n","\n","\t\t# (CONV => RELU => BN) * 3 => POOL layer set\n","\t\tConv2D(64, (3, 3), padding=\"same\"),\n","\t\tActivation(\"relu\"),\n","\t\tBatchNormalization(axis=chanDim),\n","\t\tConv2D(64, (3, 3), padding=\"same\"),\n","\t\tActivation(\"relu\"),\n","\t\tBatchNormalization(axis=chanDim),\n","\t\tConv2D(64, (3, 3), padding=\"same\"),\n","\t\tActivation(\"relu\"),\n","\t\tBatchNormalization(axis=chanDim),\n","\t\tMaxPooling2D(pool_size=(2, 2)),\n","\n","\t\t# first (and only) set of FC => RELU layers\n","\t\tFlatten(),\n","\t\tDense(256),\n","\t\tActivation(\"relu\"),\n","\t\tBatchNormalization(),\n","\t\tDropout(0.5),\n","\n","\t\t# softmax classifier\n","\t\tDense(classes),\n","\t\tActivation(\"softmax\")\n","\t])\n","\n","\t# return the built model to the calling function\n","\treturn model\n","\n","def step(X, y):\n","\t# keep track of our gradients\n","\twith tf.GradientTape() as tape:\n","\t\t# make a prediction using the model and then calculate the\n","\t\t# loss\n","\t\tpred = model(X)\n","\t\tloss = categorical_crossentropy(y, pred)\n","\n","\t# calculate the gradients using our tape and then update the\n","\t# model weights\n","\tgrads = tape.gradient(loss, model.trainable_variables)\n","\topt.apply_gradients(zip(grads, model.trainable_variables))\n","\n","# initialize the number of epochs to train for, batch size, and\n","# initial learning rate\n","EPOCHS = 25\n","BS = 64\n","INIT_LR = 1e-3\n","\n","# load the MNIST dataset\n","print(\"[INFO] loading dataset...\")\n","#((trainX, trainY), (testX, testY)) = mnist.load_data()\n","train_datagen = ImageDataGenerator( \n","\t\t\t\trescale = 1. / 255, \n","\t\t\t\tshear_range = 0.2, \n","\t\t\t\tzoom_range = 0.2, \n","\t\t\thorizontal_flip = True) \n","\n","test_datagen = ImageDataGenerator(rescale = 1. / 255) \n","\n","train_generator = train_datagen.flow_from_directory(train_data_dir, \n","\t\t\t\t\t\t\ttarget_size =(img_width, img_height), \n","\t\t\t\t\tbatch_size = batch_size)\n","\n","validation_generator = test_datagen.flow_from_directory(\n","        validation_data_dir,target_size =(img_width, img_height), \n","\t\tbatch_size = batch_size) \n","\n","trainX = train_generator\n","testX = validation_generator\n","\n","# add a channel dimension to every image in the dataset, then scale\n","# the pixel intensities to the range [0, 1]\n","trainX = np.expand_dims(trainX, axis=-1)\n","testX = np.expand_dims(testX, axis=-1)\n","trainX = trainX.astype(\"float32\") / 255.0\n","testX = testX.astype(\"float32\") / 255.0\n","\n","# one-hot encode the labels\n","trainY = to_categorical(trainY, 6)\n","testY = to_categorical(testY, 6)\n","\n","# build our model and initialize our optimizer\n","print(\"[INFO] creating model...\")\n","model = build_model(224, 224, 3, 6)\n","opt = Adam(lr=INIT_LR, decay=INIT_LR / EPOCHS)\n","\n","# compute the number of batch updates per epoch\n","numUpdates = int(trainX.shape[0] / BS)\n","\n","# loop over the number of epochs\n","for epoch in range(0, EPOCHS):\n","\t# show the current epoch number\n","\tprint(\"[INFO] starting epoch {}/{}...\".format(\n","\t\tepoch + 1, EPOCHS), end=\"\")\n","\tsys.stdout.flush()\n","\tepochStart = time.time()\n","\n","\t# loop over the data in batch size increments\n","\tfor i in range(0, numUpdates):\n","\t\t# determine starting and ending slice indexes for the current\n","\t\t# batch\n","\t\tstart = i * BS\n","\t\tend = start + BS\n","\n","\t\t# take a step\n","\t\tstep(trainX[start:end], trainY[start:end])\n","\n","\t# show timing information for the epoch\n","\tepochEnd = time.time()\n","\telapsed = (epochEnd - epochStart) / 60.0\n","\tprint(\"took {:.4} minutes\".format(elapsed))\n","\n","# in order to calculate accuracy using Keras' functions we first need\n","# to compile the model\n","model.compile(optimizer=opt, loss=categorical_crossentropy,\n","\tmetrics=[\"acc\"])\n","\n","# now that the model is compiled we can compute the accuracy\n","(loss, acc) = model.evaluate(testX, testY)\n","print(\"[INFO] test accuracy: {:.4f}\".format(acc))\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[INFO] loading dataset...\n","Found 1200 images belonging to 6 classes.\n","Found 300 images belonging to 6 classes.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"9Kvr4YTrbYPR","colab_type":"code","colab":{}},"source":["!unzip \"/content/drive/My Drive/data.zip\"\n"],"execution_count":null,"outputs":[]}]}